{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalando biblioteca minio python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting minio\n",
      "  Downloading minio-7.2.5-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi (from minio)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting urllib3 (from minio)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting argon2-cffi (from minio)\n",
      "  Downloading argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pycryptodome (from minio)\n",
      "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/galaxia/anaconda3/envs/pyminio/lib/python3.12/site-packages (from minio) (4.11.0)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi->minio)\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi->minio)\n",
      "  Downloading cffi-1.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading minio-7.2.5-py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m863.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (477 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m477.6/477.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, pycryptodome, pycparser, certifi, cffi, argon2-cffi-bindings, argon2-cffi, minio\n",
      "Successfully installed argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 certifi-2024.2.2 cffi-1.16.0 minio-7.2.5 pycparser-2.22 pycryptodome-3.20.0 urllib3-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "\n",
    "client = Minio(\n",
    "    \"localhost:9000\",\n",
    "    access_key=\"dev_user_01\",\n",
    "    secret_key=\"devpassword01\",\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "bucket_name = \"test-bucket\"\n",
    "\n",
    "found = client.bucket_exists(bucket_name)\n",
    "if not found:\n",
    "    client.make_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando arquivo ao bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.helpers.ObjectWriteResult at 0x797e79754410>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_file = 'test.txt'\n",
    "source_file = './data/test.txt'\n",
    "\n",
    "client.fput_object(bucket_name, destination_file, source_file,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalando Pyspark ao ambiente Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esse ambiente de desenvolvimento do projeto está sendo utilizado o conda como administrador de pacotes e ambientes, optaremos por instalar o pyspark via conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 24.3.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.3.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/galaxia/anaconda3/envs/pyminio\n",
      "\n",
      "  added / updated specs:\n",
      "    - pyspark\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    abseil-cpp-20211102.0      |       h27087fc_1         1.1 MB  conda-forge\n",
      "    arrow-cpp-14.0.2           |       h374c478_1        11.7 MB\n",
      "    aws-c-auth-0.6.19          |       h5eee18b_0          99 KB\n",
      "    aws-c-cal-0.5.20           |       hdbd6064_0          42 KB\n",
      "    aws-c-common-0.8.5         |       h5eee18b_0         207 KB\n",
      "    aws-c-compression-0.2.16   |       h5eee18b_0          18 KB\n",
      "    aws-c-event-stream-0.2.15  |       h6a678d5_0          50 KB\n",
      "    aws-c-http-0.6.25          |       h5eee18b_0         200 KB\n",
      "    aws-c-io-0.13.10           |       h5eee18b_0         150 KB\n",
      "    aws-c-mqtt-0.7.13          |       h5eee18b_0          67 KB\n",
      "    aws-c-s3-0.1.51            |       hdbd6064_0          71 KB\n",
      "    aws-c-sdkutils-0.1.6       |       h5eee18b_0          50 KB\n",
      "    aws-checksums-0.1.13       |       h5eee18b_0          48 KB\n",
      "    aws-crt-cpp-0.18.16        |       h6a678d5_0         213 KB\n",
      "    aws-sdk-cpp-1.10.55        |       h721c034_0         2.5 MB\n",
      "    boost-cpp-1.78.0           |       he72f1d9_0        17.1 MB  conda-forge\n",
      "    bottleneck-1.3.7           |  py312ha883a20_0         140 KB\n",
      "    gflags-2.2.2               |    he1b5a44_1004         114 KB  conda-forge\n",
      "    glog-0.5.0                 |       h48cff8f_0         104 KB  conda-forge\n",
      "    icu-70.1                   |       h27087fc_0        13.5 MB  conda-forge\n",
      "    intel-openmp-2023.1.0      |   hdb19cb5_46306        17.2 MB\n",
      "    libbrotlicommon-1.0.9      |       h166bdaf_7          65 KB  conda-forge\n",
      "    libbrotlidec-1.0.9         |       h166bdaf_7          33 KB  conda-forge\n",
      "    libbrotlienc-1.0.9         |       h166bdaf_7         287 KB  conda-forge\n",
      "    libcurl-8.5.0              |       h251f7ec_0         416 KB\n",
      "    libedit-3.1.20230828       |       h5eee18b_0         179 KB\n",
      "    libev-4.33                 |       h516909a_1         104 KB  conda-forge\n",
      "    libnghttp2-1.57.0          |       h2d74bed_0         674 KB\n",
      "    mkl-2023.1.0               |   h213fc3f_46344       171.5 MB\n",
      "    mkl-service-2.4.0          |  py312h5eee18b_1          66 KB\n",
      "    mkl_fft-1.3.8              |  py312h5eee18b_0         204 KB\n",
      "    mkl_random-1.2.4           |  py312hdb19cb5_0         284 KB\n",
      "    numexpr-2.8.7              |  py312hf827012_0         149 KB\n",
      "    numpy-1.26.4               |  py312hc5e2394_0          11 KB\n",
      "    numpy-base-1.26.4          |  py312h0da6c21_0         7.7 MB\n",
      "    pandas-2.2.1               |  py312h526ad5a_0        15.4 MB\n",
      "    py4j-0.10.9.7              |     pyhd8ed1ab_0         182 KB  conda-forge\n",
      "    pyarrow-14.0.2             |  py312hb107042_0         4.6 MB\n",
      "    pyspark-3.5.1              |     pyhd8ed1ab_0       296.3 MB  conda-forge\n",
      "    python-tzdata-2024.1       |     pyhd8ed1ab_0         141 KB  conda-forge\n",
      "    pytz-2024.1                |     pyhd8ed1ab_0         184 KB  conda-forge\n",
      "    re2-2022.04.01             |       h27087fc_0         212 KB  conda-forge\n",
      "    s2n-1.3.27                 |       hdbd6064_0         382 KB\n",
      "    snappy-1.1.10              |       h6a678d5_1          43 KB\n",
      "    utf8proc-2.6.1             |       h5eee18b_1          93 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       563.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  abseil-cpp         conda-forge/linux-64::abseil-cpp-20211102.0-h27087fc_1 \n",
      "  arrow-cpp          pkgs/main/linux-64::arrow-cpp-14.0.2-h374c478_1 \n",
      "  aws-c-auth         pkgs/main/linux-64::aws-c-auth-0.6.19-h5eee18b_0 \n",
      "  aws-c-cal          pkgs/main/linux-64::aws-c-cal-0.5.20-hdbd6064_0 \n",
      "  aws-c-common       pkgs/main/linux-64::aws-c-common-0.8.5-h5eee18b_0 \n",
      "  aws-c-compression  pkgs/main/linux-64::aws-c-compression-0.2.16-h5eee18b_0 \n",
      "  aws-c-event-stream pkgs/main/linux-64::aws-c-event-stream-0.2.15-h6a678d5_0 \n",
      "  aws-c-http         pkgs/main/linux-64::aws-c-http-0.6.25-h5eee18b_0 \n",
      "  aws-c-io           pkgs/main/linux-64::aws-c-io-0.13.10-h5eee18b_0 \n",
      "  aws-c-mqtt         pkgs/main/linux-64::aws-c-mqtt-0.7.13-h5eee18b_0 \n",
      "  aws-c-s3           pkgs/main/linux-64::aws-c-s3-0.1.51-hdbd6064_0 \n",
      "  aws-c-sdkutils     pkgs/main/linux-64::aws-c-sdkutils-0.1.6-h5eee18b_0 \n",
      "  aws-checksums      pkgs/main/linux-64::aws-checksums-0.1.13-h5eee18b_0 \n",
      "  aws-crt-cpp        pkgs/main/linux-64::aws-crt-cpp-0.18.16-h6a678d5_0 \n",
      "  aws-sdk-cpp        pkgs/main/linux-64::aws-sdk-cpp-1.10.55-h721c034_0 \n",
      "  blas               pkgs/main/linux-64::blas-1.0-mkl \n",
      "  boost-cpp          conda-forge/linux-64::boost-cpp-1.78.0-he72f1d9_0 \n",
      "  bottleneck         pkgs/main/linux-64::bottleneck-1.3.7-py312ha883a20_0 \n",
      "  c-ares             pkgs/main/linux-64::c-ares-1.19.1-h5eee18b_0 \n",
      "  gflags             conda-forge/linux-64::gflags-2.2.2-he1b5a44_1004 \n",
      "  glog               conda-forge/linux-64::glog-0.5.0-h48cff8f_0 \n",
      "  grpc-cpp           pkgs/main/linux-64::grpc-cpp-1.48.2-he1ff14a_1 \n",
      "  icu                conda-forge/linux-64::icu-70.1-h27087fc_0 \n",
      "  intel-openmp       pkgs/main/linux-64::intel-openmp-2023.1.0-hdb19cb5_46306 \n",
      "  krb5               pkgs/main/linux-64::krb5-1.20.1-h143b758_1 \n",
      "  libbrotlicommon    conda-forge/linux-64::libbrotlicommon-1.0.9-h166bdaf_7 \n",
      "  libbrotlidec       conda-forge/linux-64::libbrotlidec-1.0.9-h166bdaf_7 \n",
      "  libbrotlienc       conda-forge/linux-64::libbrotlienc-1.0.9-h166bdaf_7 \n",
      "  libcurl            pkgs/main/linux-64::libcurl-8.5.0-h251f7ec_0 \n",
      "  libedit            pkgs/main/linux-64::libedit-3.1.20230828-h5eee18b_0 \n",
      "  libev              conda-forge/linux-64::libev-4.33-h516909a_1 \n",
      "  libevent           pkgs/main/linux-64::libevent-2.1.12-hdbd6064_1 \n",
      "  libnghttp2         pkgs/main/linux-64::libnghttp2-1.57.0-h2d74bed_0 \n",
      "  libprotobuf        pkgs/main/linux-64::libprotobuf-3.20.3-he621ea3_0 \n",
      "  libssh2            pkgs/main/linux-64::libssh2-1.10.0-hdbd6064_2 \n",
      "  libthrift          pkgs/main/linux-64::libthrift-0.15.0-h1795dd8_2 \n",
      "  lz4-c              pkgs/main/linux-64::lz4-c-1.9.4-h6a678d5_0 \n",
      "  mkl                pkgs/main/linux-64::mkl-2023.1.0-h213fc3f_46344 \n",
      "  mkl-service        pkgs/main/linux-64::mkl-service-2.4.0-py312h5eee18b_1 \n",
      "  mkl_fft            pkgs/main/linux-64::mkl_fft-1.3.8-py312h5eee18b_0 \n",
      "  mkl_random         pkgs/main/linux-64::mkl_random-1.2.4-py312hdb19cb5_0 \n",
      "  numexpr            pkgs/main/linux-64::numexpr-2.8.7-py312hf827012_0 \n",
      "  numpy              pkgs/main/linux-64::numpy-1.26.4-py312hc5e2394_0 \n",
      "  numpy-base         pkgs/main/linux-64::numpy-base-1.26.4-py312h0da6c21_0 \n",
      "  orc                pkgs/main/linux-64::orc-1.7.4-hb3bc3d3_1 \n",
      "  pandas             pkgs/main/linux-64::pandas-2.2.1-py312h526ad5a_0 \n",
      "  py4j               conda-forge/noarch::py4j-0.10.9.7-pyhd8ed1ab_0 \n",
      "  pyarrow            pkgs/main/linux-64::pyarrow-14.0.2-py312hb107042_0 \n",
      "  pyspark            conda-forge/noarch::pyspark-3.5.1-pyhd8ed1ab_0 \n",
      "  python-tzdata      conda-forge/noarch::python-tzdata-2024.1-pyhd8ed1ab_0 \n",
      "  pytz               conda-forge/noarch::pytz-2024.1-pyhd8ed1ab_0 \n",
      "  re2                conda-forge/linux-64::re2-2022.04.01-h27087fc_0 \n",
      "  s2n                pkgs/main/linux-64::s2n-1.3.27-hdbd6064_0 \n",
      "  snappy             pkgs/main/linux-64::snappy-1.1.10-h6a678d5_1 \n",
      "  tbb                pkgs/main/linux-64::tbb-2021.8.0-hdb19cb5_0 \n",
      "  utf8proc           pkgs/main/linux-64::utf8proc-2.6.1-h5eee18b_1 \n",
      "  zstd               pkgs/main/linux-64::zstd-1.5.5-hc292b87_0 \n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_csvs():\n",
    "    url_list = []\n",
    "    for i in range(18, 21):\n",
    "        url_first_semester = f\"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/dsas/ca/ca-20{str(i)}-01.csv\"\n",
    "        url_second_semester = f\"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/dsas/ca/ca-20{str(i)}-02.csv\"\n",
    "        url_list.append(url_first_semester)\n",
    "        url_list.append(url_second_semester)\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csvs(url_list):\n",
    "    df_list = []\n",
    "    for url in url_list:\n",
    "        try:\n",
    "            with requests.Session() as s:\n",
    "                decoded_content = s.get(url).content.decode('utf-8')\n",
    "                downloaded_csv = csv.reader(decoded_content.splitlines(), delimiter=';')\n",
    "                df_list.append(pd.DataFrame(list(downloaded_csv)))\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading csv: {e}\")\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    final_df.to_csv('./data/final_df.csv', index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/24 00:15:16 WARN Utils: Your hostname, galaxia-Vostro-3520 resolves to a loopback address: 127.0.1.1; using 192.168.10.57 instead (on interface wlp0s20f3)\n",
      "24/04/24 00:15:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/24 00:15:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/dsas/ca/ca-2018-01.csv', 'https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/dsas/ca/ca-2018-02.csv', 'https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/dsas/ca/ca-2019-01.csv', 'https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/dsas/ca/ca-2019-02.csv', 'https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/dsas/ca/ca-2020-01.csv', 'https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/dsas/ca/ca-2020-02.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2216503"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "spark = SparkSession.builder.appName('minio-test').getOrCreate()\n",
    "web_dataset = search_csvs()\n",
    "print(web_dataset)\n",
    "#download_csvs(web_dataset)\n",
    "dataset = './data/final_df.csv'\n",
    "gasoline_df = spark.read.format('csv').option('header', 'true').load(dataset)\n",
    "\n",
    "gasoline_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"dev_user_01\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"devpassword01\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o67.parquet.\n: java.lang.NoClassDefFoundError: software/amazon/awssdk/awscore/exception/AwsServiceException\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2625)\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2590)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: software.amazon.awssdk.awscore.exception.AwsServiceException\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 30 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[43mgasoline_df\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/gasoline_state.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyminio/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyminio/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pyminio/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyminio/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o67.parquet.\n: java.lang.NoClassDefFoundError: software/amazon/awssdk/awscore/exception/AwsServiceException\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2625)\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2590)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: software.amazon.awssdk.awscore.exception.AwsServiceException\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 30 more\n"
     ]
    }
   ],
   "source": [
    "gasoline_df \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"s3a://{bucket_name}/gasoline_state.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyminio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
